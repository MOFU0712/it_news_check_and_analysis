{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMswr73A2wviogKuVUm/FUa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MOFU0712/it_news_check_and_analysis/blob/main/it_news_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZfAjHVzcI0LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1iuVqg8G8G1"
      },
      "outputs": [],
      "source": [
        "!pip3 install japanize_matplotlib\n",
        "\n",
        "# ワードクラウド生成用\n",
        "!pip3 install wordcloud\n",
        "\n",
        "# 日本語テキスト処理用\n",
        "!pip3 install mecab-python3 unidic-lite\n",
        "\n",
        "# 自然言語処理用\n",
        "!pip3 install nltk\n",
        "\n",
        "# ネットワークグラフ可視化用\n",
        "!pip3 install networkx\n",
        "\n",
        "# Notionとの接続\n",
        "!pip3 install notion_client\n",
        "\n",
        "# フォントのインストール\n",
        "!apt-get -qq update\n",
        "!apt-get -qq install -y fonts-ipafont-gothic fonts-ipafont-mincho"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "from collections import Counter\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.font_manager as fm\n",
        "from matplotlib.font_manager import FontProperties\n",
        "import numpy as np\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import MeCab\n",
        "import networkx as nx\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import japanize_matplotlib\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# 日本語フォントプロパティの設定\n",
        "font_prop = FontProperties(fname=font_path)\n",
        "\n",
        "# Colabのシークレットから認証情報を取得\n",
        "NOTION_API_KEY = userdata.get('NOTION_API_KEY')\n",
        "DATABASE_ID = userdata.get('PICKUP_DATABASE_KEY')\n",
        "\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {NOTION_API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Notion-Version\": \"2022-06-28\"\n",
        "}\n",
        "\n",
        "def get_database_schema():\n",
        "    \"\"\"データベースのスキーマを取得する\"\"\"\n",
        "    url = f\"https://api.notion.com/v1/databases/{DATABASE_ID}\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    return response.json()\n",
        "\n",
        "def query_database(start_date=None, end_date=None):\n",
        "    \"\"\"指定された期間のデータベースエントリを取得する\"\"\"\n",
        "    url = f\"https://api.notion.com/v1/databases/{DATABASE_ID}/query\"\n",
        "\n",
        "    # フィルタの設定\n",
        "    filter_params = {}\n",
        "    if start_date and end_date:\n",
        "        filter_params = {\n",
        "            \"and\": [\n",
        "                {\n",
        "                    \"property\": \"Created time\",\n",
        "                    \"date\": {\n",
        "                        \"on_or_after\": start_date\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"property\": \"Created time\",\n",
        "                    \"date\": {\n",
        "                        \"on_or_before\": end_date\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    # ページネーションのための変数\n",
        "    has_more = True\n",
        "    next_cursor = None\n",
        "    results = []\n",
        "\n",
        "    # ページネーションで全結果を取得\n",
        "    while has_more:\n",
        "        body = {\"page_size\": 100}\n",
        "        if filter_params:\n",
        "            body[\"filter\"] = filter_params\n",
        "        if next_cursor:\n",
        "            body[\"start_cursor\"] = next_cursor\n",
        "\n",
        "        response = requests.post(url, headers=HEADERS, json=body)\n",
        "        data = response.json()\n",
        "\n",
        "        results.extend(data.get(\"results\", []))\n",
        "        has_more = data.get(\"has_more\", False)\n",
        "        next_cursor = data.get(\"next_cursor\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def extract_properties(results):\n",
        "    \"\"\"Notionの結果からプロパティを抽出してDataFrameに変換する\"\"\"\n",
        "    articles = []\n",
        "\n",
        "    for result in results:\n",
        "        properties = result.get(\"properties\", {})\n",
        "        article = {}\n",
        "\n",
        "        # タイトルを取得 (name)\n",
        "        title_prop = properties.get(\"name\", {})\n",
        "        if title_prop and title_prop.get(\"title\"):\n",
        "            title_text = [text.get(\"plain_text\", \"\") for text in title_prop.get(\"title\", [])]\n",
        "            article[\"title\"] = \"\".join(title_text)\n",
        "\n",
        "        # 作成日時を取得 (Created time)\n",
        "        created_time_prop = properties.get(\"Created time\", {})\n",
        "        if created_time_prop:\n",
        "            article[\"date\"] = created_time_prop.get(\"created_time\", \"\")\n",
        "\n",
        "        # タグを取得 (tag - select型)\n",
        "        tag_prop = properties.get(\"tag\", {})\n",
        "        if tag_prop and tag_prop.get(\"select\"):\n",
        "            article[\"category\"] = tag_prop.get(\"select\", {}).get(\"name\", \"\")\n",
        "\n",
        "        # ピックアップタイプを取得 (pickup_type - select型)\n",
        "        pickup_type_prop = properties.get(\"pickup_type\", {})\n",
        "        if pickup_type_prop and pickup_type_prop.get(\"select\"):\n",
        "            pickup_type = pickup_type_prop.get(\"select\", {}).get(\"name\", \"\")\n",
        "            if pickup_type:\n",
        "                article[\"pickup_type\"] = pickup_type\n",
        "\n",
        "        # 要約コンテンツを取得 (summary - rich_text型)\n",
        "        summary_prop = properties.get(\"summary\", {})\n",
        "        if summary_prop and summary_prop.get(\"rich_text\"):\n",
        "            summary_text = [text.get(\"plain_text\", \"\") for text in summary_prop.get(\"rich_text\", [])]\n",
        "            article[\"content\"] = \"\".join(summary_text)\n",
        "\n",
        "        # abstract情報も取得 (abstract - rich_text型)\n",
        "        abstract_prop = properties.get(\"abstract\", {})\n",
        "        if abstract_prop and abstract_prop.get(\"rich_text\"):\n",
        "            abstract_text = [text.get(\"plain_text\", \"\") for text in abstract_prop.get(\"rich_text\", [])]\n",
        "            # contentがすでにある場合は追加、ない場合は新規設定\n",
        "            if \"content\" in article and article[\"content\"]:\n",
        "                article[\"content\"] += \" \" + \"\".join(abstract_text)\n",
        "            else:\n",
        "                article[\"content\"] = \"\".join(abstract_text)\n",
        "\n",
        "\n",
        "        # URLを取得 (URL - url型)\n",
        "        url_prop = properties.get(\"URL\", {})\n",
        "        if url_prop:\n",
        "            article[\"url\"] = url_prop.get(\"url\", \"\")\n",
        "\n",
        "        articles.append(article)\n",
        "\n",
        "    df = pd.DataFrame(articles)\n",
        "\n",
        "    return df\n",
        "\n",
        "def setup_japanese_fonts():\n",
        "    \"\"\"日本語フォントをインストールして設定する\"\"\"\n",
        "    try:\n",
        "\n",
        "        # フォントファイルのパスを確認\n",
        "        font_path = '/usr/share/fonts/truetype/fonts-japanese-gothic.ttf'\n",
        "        if not os.path.exists(font_path):\n",
        "            font_path = '/usr/share/fonts/truetype/ipafont-gothic/ipag.ttf'\n",
        "\n",
        "        # フォント設定\n",
        "        font_prop = fm.FontProperties(fname=font_path)\n",
        "        matplotlib.rcParams['font.family'] = font_prop.get_name()\n",
        "\n",
        "        # フォントキャッシュの再構築\n",
        "        fm._rebuild()\n",
        "\n",
        "        print(\"日本語フォントの設定が完了しました\")\n",
        "        return font_prop\n",
        "    except Exception as e:\n",
        "        print(f\"日本語フォント設定中にエラーが発生しました: {e}\")\n",
        "        return None\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"テキストの前処理を行う\"\"\"\n",
        "    # 小文字化\n",
        "    text = text.lower()\n",
        "    # 特殊文字や数字を削除\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    return text\n",
        "\n",
        "def tokenize_japanese_simple(text):\n",
        "    \"\"\"日本語テキストをシンプルに単語に分割する\"\"\"\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        # 単語分割のみを行う\n",
        "        tagger = MeCab.Tagger(\"-Owakati\")\n",
        "        words = tagger.parse(text).split()\n",
        "        return words\n",
        "    except Exception as e:\n",
        "        print(f\"形態素解析中にエラーが発生しました: {e}\")\n",
        "        # 簡易的な分割\n",
        "        return text.split()\n",
        "\n",
        "def is_likely_noun(word):\n",
        "    \"\"\"単語が名詞である可能性が高いかをヒューリスティックに判定\"\"\"\n",
        "    # 単語の長さでフィルタリング（短すぎる単語は除外）\n",
        "    if len(word) <= 1:\n",
        "        return False\n",
        "\n",
        "    # 助詞、助動詞の一般的なリスト\n",
        "    particles = ['が', 'の', 'を', 'に', 'へ', 'と', 'から', 'より', 'で', 'や', 'は', 'も', 'に', 'て', 'で', 'ば', 'なら']\n",
        "    if word in particles:\n",
        "        return False\n",
        "\n",
        "    # 動詞の終止形などの典型的なパターン\n",
        "    verb_endings = ['する', 'れる', 'られる', 'せる', 'させる', 'なる', 'ある', 'いる']\n",
        "    for ending in verb_endings:\n",
        "        if word.endswith(ending):\n",
        "            return False\n",
        "\n",
        "    # カタカナ語は技術用語である可能性が高い\n",
        "    if any(c in 'アイウエオカキクケコサシスセソタチツテトナニヌネノハヒフヘホマミムメモヤユヨラリルレロワヲンガギグゲゴザジズゼゾダヂヅデドバビブベボパピプペポ' for c in word):\n",
        "        return True\n",
        "\n",
        "    # 英数字を含む単語は技術用語である可能性が高い\n",
        "    if any(c.isascii() for c in word):\n",
        "        return True\n",
        "\n",
        "    # その他の単語は3文字以上であれば名詞の可能性が高い\n",
        "    return len(word) >= 3\n",
        "\n",
        "def create_word_frequency(df, column='content', english_only=False):\n",
        "    \"\"\"テキストから技術用語の単語頻度を計算する\"\"\"\n",
        "    stopwords = get_stopwords()\n",
        "    all_words = []\n",
        "\n",
        "    for text in df[column].fillna(\"\"):\n",
        "        if not isinstance(text, str) or not text:\n",
        "            continue\n",
        "\n",
        "        # 単語分割\n",
        "        tokens = tokenize_japanese_simple(text)\n",
        "\n",
        "        if english_only:\n",
        "            # 英語の技術用語のみをフィルタリング\n",
        "            filtered_tokens = [word for word in tokens if is_english_tech_term(word)]\n",
        "        else:\n",
        "            # 名詞である可能性が高い単語のみをフィルタリング\n",
        "            filtered_tokens = [word for word in tokens if is_likely_noun(word) and word not in stopwords]\n",
        "\n",
        "        all_words.extend(filtered_tokens)\n",
        "\n",
        "    # 単語の出現回数をカウント\n",
        "    word_freq = Counter(all_words)\n",
        "\n",
        "    # 頻度が低すぎる単語は除外（ノイズ除去）\n",
        "    word_freq = Counter({word: count for word, count in word_freq.items() if count > 1})\n",
        "\n",
        "    print(f\"抽出された{'英語' if english_only else ''}技術用語: {', '.join(list(word_freq.keys())[:20])}\")\n",
        "    print(f\"単語頻度データを生成しました（{len(word_freq)}個の単語）\")\n",
        "\n",
        "    return word_freq\n",
        "\n",
        "def get_stopwords():\n",
        "    \"\"\"拡張版日本語のストップワードを取得\"\"\"\n",
        "    # 基本的なストップワード\n",
        "    ja_stops = set(['の', 'に', 'は', 'を', 'た', 'が', 'で', 'て', 'と', 'し', 'れ', 'さ', 'ある', 'いる', 'する', 'ない',\n",
        "                     'できる', 'なる', 'もの', 'こと', 'これ', 'それ', 'あれ', 'この', 'その', 'あの', 'ます', 'です',\n",
        "                     'だ', 'した', 'して', 'しない', 'なった', 'なって', 'なる', 'なり', 'など', 'とき', 'ところ'])\n",
        "\n",
        "    # 追加の一般的な助詞・助動詞\n",
        "    ja_stops.update(['も', 'や', 'へ', 'から', 'より', 'によって', 'において', 'について', 'として',\n",
        "                    'ため', 'ための', 'ように', 'ような', 'らしい', 'れる', 'られる'])\n",
        "\n",
        "    # 一文字語\n",
        "    ja_stops.update(['ま', 'い', 'す', 'あ', 'お', 'く', 'き', 'け', 'さ', 'し', 'せ', 'そ', 'た', 'ち', 'つ', 'て', 'と',\n",
        "                    'な', 'に', 'ぬ', 'ね', 'の', 'は', 'ひ', 'ふ', 'へ', 'ほ', 'ま', 'み', 'む', 'め', 'も', 'や', 'ゆ',\n",
        "                    'よ', 'ら', 'り', 'る', 'れ', 'ろ', 'わ', 'を', 'ん'])\n",
        "\n",
        "    # 分析対象の記事内容に合わせて追加の分野特有語・助詞・単語\n",
        "    tech_common_words = [\n",
        "        'あり', 'あれ', 'いう', 'いく', 'いっ', 'おく', 'おり', 'から', 'くる', 'くれ', 'この', 'これ',\n",
        "        'させる', 'さらに', 'さまざま', 'しまう', 'しまった', 'している', 'してき', 'してきた',\n",
        "        'すべて', 'その', 'それ', 'たくさん', 'ため', 'とき', 'ところ', 'とは', 'との', 'なっ',\n",
        "        'なった', 'なって', 'なり', 'なる', 'など', 'について', 'による', 'または', 'また', 'まで',\n",
        "        'まし', 'もの', 'よう', 'より', 'られ', 'られる', 'れた', 'れて', 'れる', 'わかる',\n",
        "        'でき', 'できる', 'でも', 'という', 'への', 'ほとんど', 'もの', 'やすい', 'よる',\n",
        "        'いる', 'ある', 'する', 'せる', 'こと', 'いくつか', 'どの', 'どう', 'どんな',\n",
        "\n",
        "        # 技術単語\n",
        "        'ソース', 'モード', '性能', '画像', '投稿', '活用', '可能', '利用', '提供', '開発',\n",
        "        '導入', '機能', '発表', '公開', '対応', '実現', '向け', '向上', '必要', '課題', '問題',\n",
        "        '方法', '取り組み', '研究', '技術', '企業', '情報', '最新', '新た', '新しい', '重要',\n",
        "        '多く', '生成', '予定', '自動', '簡単', '強化', '画像', '使用', '目的', '特徴', '改善'\n",
        "    ]\n",
        "\n",
        "    ja_stops.update(tech_common_words)\n",
        "\n",
        "    return ja_stops\n",
        "\n",
        "\n",
        "def generate_wordcloud(word_freq, title, output_file, max_words=100):\n",
        "    \"\"\"ワードクラウドを生成して保存する\"\"\"\n",
        "    if not word_freq:\n",
        "        print(\"単語頻度データが空のため、ワードクラウドを生成できません\")\n",
        "        return\n",
        "\n",
        "    # カラーマップの設定\n",
        "    colors = [\"#66c2a5\", \"#fc8d62\", \"#8da0cb\", \"#e78ac3\", \"#a6d854\"]\n",
        "    cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors, N=100)\n",
        "\n",
        "    # 日本語フォントの設定（Colab用）\n",
        "    try:\n",
        "        # 日本語フォントをインストール\n",
        "        import subprocess\n",
        "        subprocess.run(['apt-get', 'update'], check=True)\n",
        "        subprocess.run(['apt-get', 'install', '-y', 'fonts-noto-cjk'], check=True)\n",
        "\n",
        "        # フォントパスを直接指定\n",
        "        font_path = '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc'\n",
        "\n",
        "        wordcloud = WordCloud(\n",
        "            font_path=font_path,\n",
        "            width=800,\n",
        "            height=400,\n",
        "            background_color='white',\n",
        "            colormap=cmap,\n",
        "            max_words=max_words,\n",
        "            collocations=False\n",
        "        ).generate_from_frequencies(word_freq)\n",
        "    except Exception as e:\n",
        "        print(f\"日本語フォント設定エラー: {e}\")\n",
        "        print(\"フォントなしでワードクラウドを生成します\")\n",
        "        # フォントパスなしでワードクラウドを生成\n",
        "        wordcloud = WordCloud(\n",
        "            width=800,\n",
        "            height=400,\n",
        "            background_color='white',\n",
        "            colormap=cmap,\n",
        "            max_words=100,\n",
        "            collocations=False,\n",
        "            font_path=None\n",
        "        ).generate_from_frequencies(word_freq)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(title, fontsize=15)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"ワードクラウドを保存しました: {output_file}\")\n",
        "\n",
        "def create_topic_model_sklearn(df, column='content', num_topics=5, english_only=False):\n",
        "    \"\"\"トピックモデリング（LDA）をscikit-learnで実行する - 英語フィルタリング対応版\"\"\"\n",
        "    if column not in df.columns or df[column].isna().all():\n",
        "        print(f\"カラム '{column}' が存在しないか、すべての値が空です\")\n",
        "        return {}, None, None, None\n",
        "\n",
        "    try:\n",
        "        # テキストを前処理\n",
        "        stopwords = get_stopwords()\n",
        "        processed_docs = []\n",
        "\n",
        "        for text in df[column].fillna(\"\"):\n",
        "            if not isinstance(text, str) or not text:\n",
        "                processed_docs.append(\"\")\n",
        "                continue\n",
        "\n",
        "            # 単語分割\n",
        "            tokens = tokenize_japanese_simple(text)\n",
        "\n",
        "            if english_only:\n",
        "                # 英語の技術用語のみをフィルタリング\n",
        "                filtered_tokens = [word for word in tokens if is_english_tech_term(word)]\n",
        "            else:\n",
        "                # 名詞である可能性が高い単語のみをフィルタリング\n",
        "                filtered_tokens = [word for word in tokens if is_likely_noun(word) and word not in stopwords]\n",
        "\n",
        "            processed_docs.append(\" \".join(filtered_tokens))\n",
        "\n",
        "        # 空のドキュメントをフィルタリング\n",
        "        processed_docs = [doc for doc in processed_docs if doc]\n",
        "\n",
        "        if not processed_docs:\n",
        "            print(\"有効なドキュメントがありません\")\n",
        "            return {}, None, None, None\n",
        "\n",
        "        # トピック数の調整（データが少ない場合）\n",
        "        if len(processed_docs) < num_topics:\n",
        "            num_topics = max(2, min(5, len(processed_docs) // 2))\n",
        "            print(f\"データ数に合わせてトピック数を {num_topics} に調整しました\")\n",
        "\n",
        "        # CountVectorizerでBoW (Bag of Words) を作成\n",
        "        vectorizer = CountVectorizer(max_features=1000)\n",
        "        X = vectorizer.fit_transform(processed_docs)\n",
        "\n",
        "        if X.shape[1] == 0:\n",
        "            print(\"抽出された特徴量がありません\")\n",
        "            return {}, None, None, None\n",
        "\n",
        "        # LDAモデルを訓練\n",
        "        lda = LatentDirichletAllocation(\n",
        "            n_components=num_topics,\n",
        "            random_state=42,\n",
        "            max_iter=10,\n",
        "            learning_method='online'\n",
        "        )\n",
        "        lda.fit(X)\n",
        "\n",
        "        # 特徴量（単語）の名前を取得\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "        # トピックごとの上位単語を抽出\n",
        "        topics = {}\n",
        "        for idx, topic in enumerate(lda.components_):\n",
        "            top_words_idx = topic.argsort()[:-21:-1]  # 上位20語のインデックスを取得（より多くの単語を取得）\n",
        "            top_words = [feature_names[i] for i in top_words_idx]\n",
        "            topics[f\"Topic {idx+1}\"] = top_words\n",
        "\n",
        "        # 各ドキュメントのトピック分布を取得\n",
        "        doc_topic_dist = lda.transform(X)\n",
        "\n",
        "        return topics, doc_topic_dist, vectorizer, lda\n",
        "    except Exception as e:\n",
        "        print(f\"トピックモデリング中にエラーが発生しました: {e}\")\n",
        "        return {}, None, None, None\n",
        "def plot_topic_distribution_sklearn(doc_topic_dist, topics, num_topics, output_file):\n",
        "    \"\"\"記事のトピック分布を可視化する（scikit-learn版） - 改良版\"\"\"\n",
        "    if doc_topic_dist is None or doc_topic_dist.size == 0 or not topics:\n",
        "        print(\"トピック分布データがないため、グラフを生成できません\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # 各ドキュメントの主要トピックを取得\n",
        "        dominant_topics = np.argmax(doc_topic_dist, axis=1)\n",
        "\n",
        "        # トピック分布のカウント\n",
        "        topic_counts = Counter(dominant_topics)\n",
        "\n",
        "        # トピックの表示名を作成（上位3単語を含める）\n",
        "        topic_labels = []\n",
        "        for i in range(num_topics):\n",
        "            if f\"Topic {i+1}\" in topics and topics[f\"Topic {i+1}\"]:\n",
        "                top_words = topics[f\"Topic {i+1}\"][:3]  # 上位3単語\n",
        "                label = f\"Topic {i+1}\\n({', '.join(top_words)})\"\n",
        "            else:\n",
        "                label = f\"Topic {i+1}\"\n",
        "            topic_labels.append(label)\n",
        "\n",
        "        counts = [topic_counts.get(i, 0) for i in range(num_topics)]\n",
        "\n",
        "        # 棒グラフの作成\n",
        "        plt.figure(figsize=(12, 7))\n",
        "        ax = sns.barplot(x=topic_labels, y=counts)\n",
        "        plt.title(\"記事のトピック分布\", fontsize=16)\n",
        "        plt.xlabel(\"トピック（上位3キーワード）\", fontsize=14)\n",
        "        plt.ylabel(\"記事数\", fontsize=14)\n",
        "        plt.xticks(rotation=15, ha='right')\n",
        "\n",
        "        # バーの上に数値を表示\n",
        "        for i, v in enumerate(counts):\n",
        "            ax.text(i, v + 0.5, str(v), ha='center')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"トピック分布図を保存しました: {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"トピック分布図の作成中にエラーが発生しました: {e}\")\n",
        "\n",
        "def plot_category_distribution(df, output_file):\n",
        "    \"\"\"タグ（カテゴリ）の分布を可視化する \"\"\"\n",
        "    if 'category' not in df.columns or df['category'].isna().all():\n",
        "        print(\"カテゴリデータがないため、カテゴリ分布図を生成できません\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # 空の値を除外\n",
        "        category_counts = df['category'].dropna().value_counts()\n",
        "\n",
        "        if len(category_counts) == 0:\n",
        "            print(\"有効なカテゴリがないため、カテゴリ分布図を生成できません\")\n",
        "            return\n",
        "\n",
        "        # 多すぎる場合は上位20個に制限\n",
        "        if len(category_counts) > 20:\n",
        "            category_counts = category_counts.head(20)\n",
        "            print(f\"カテゴリが多いため、上位20個のみを表示します\")\n",
        "\n",
        "        plt.figure(figsize=(20, 8))\n",
        "        ax = sns.barplot(x=category_counts.index, y=category_counts.values)\n",
        "        plt.title(\"記事のタグ分布 (上位20件)\", fontsize=16)\n",
        "        plt.xlabel(\"タグ\", fontsize=14)\n",
        "        plt.ylabel(\"記事数\", fontsize=14)\n",
        "\n",
        "        # x軸ラベルの回転とレイアウト調整\n",
        "        plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "        plt.subplots_adjust(bottom=0.3)  # 下部の余白を増やす\n",
        "\n",
        "        # バーの上に数値を表示\n",
        "        for i, v in enumerate(category_counts.values):\n",
        "            ax.text(i, v + 0.5, str(v), ha='center')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"カテゴリ分布図を保存しました: {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"カテゴリ分布図の作成中にエラーが発生しました: {e}\")\n",
        "\n",
        "def plot_time_series(df, output_file):\n",
        "    \"\"\"時系列での記事数の推移を可視化する\"\"\"\n",
        "    if 'date' not in df.columns:\n",
        "        return\n",
        "\n",
        "    # 日付をdatetime型に変換\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    # 日付ごとに記事数を集計\n",
        "    daily_counts = df.groupby(df['date'].dt.date).size()\n",
        "\n",
        "    # 折れ線グラフの作成\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(daily_counts.index, daily_counts.values, marker='o', linestyle='-', color='#3498db')\n",
        "    plt.title(\"日別記事数の推移\", fontsize=15)\n",
        "    plt.xlabel(\"日付\", fontsize=12)\n",
        "    plt.ylabel(\"記事数\", fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def analyze_category_co_occurrence(df, output_file):\n",
        "    \"\"\"カテゴリの共起関係を分析して可視化する\"\"\"\n",
        "    if 'category' not in df.columns or df['category'].isna().all():\n",
        "        print(\"categoryカラムがないか空のため、カテゴリネットワーク図を生成できません\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "\n",
        "\n",
        "        # カテゴリの出現頻度をカウント\n",
        "        category_counts = df['category'].value_counts().to_dict()\n",
        "\n",
        "        # 共起関係を調べるため、同じ日に出現したカテゴリのペアを作成\n",
        "        df['date'] = pd.to_datetime(df['date']).dt.date\n",
        "        category_pairs = []\n",
        "\n",
        "        for date, group in df.groupby('date'):\n",
        "            categories = group['category'].dropna().unique()\n",
        "            for i, cat1 in enumerate(categories):\n",
        "                for cat2 in categories[i+1:]:\n",
        "                    pair = tuple(sorted([cat1, cat2]))\n",
        "                    category_pairs.append(pair)\n",
        "\n",
        "        category_co_occurrence = Counter(category_pairs)\n",
        "\n",
        "        # 共起頻度が多い上位のカテゴリペアを取得\n",
        "        top_pairs = category_co_occurrence.most_common(15)\n",
        "        if not top_pairs:\n",
        "            print(\"カテゴリの共起関係がないため、カテゴリネットワーク図を生成できません\")\n",
        "            return False\n",
        "\n",
        "        print(f\"カテゴリの共起関係: {top_pairs[:5]}\")\n",
        "\n",
        "        # 日本語フォントのパス\n",
        "        font_path = '/usr/share/fonts/truetype/fonts-japanese-gothic.ttf'\n",
        "        if not os.path.exists(font_path):\n",
        "            font_path = '/usr/share/fonts/truetype/ipafont-gothic/ipag.ttf'\n",
        "\n",
        "        # ネットワーク図のためのノードとエッジを作成\n",
        "        import networkx as nx\n",
        "        G = nx.Graph()\n",
        "\n",
        "        # すべてのカテゴリを抽出\n",
        "        all_categories = set()\n",
        "        for pair, count in top_pairs:\n",
        "            cat1, cat2 = pair\n",
        "            all_categories.add(cat1)\n",
        "            all_categories.add(cat2)\n",
        "\n",
        "        # ノードを追加（出現頻度に基づいてサイズを調整）\n",
        "        for category in all_categories:\n",
        "            G.add_node(category, size=category_counts.get(category, 1))\n",
        "\n",
        "        # エッジを追加\n",
        "        for pair, count in top_pairs:\n",
        "            cat1, cat2 = pair\n",
        "            G.add_edge(cat1, cat2, weight=count)\n",
        "\n",
        "        # ネットワーク図の作成\n",
        "        plt.figure(figsize=(14, 10))\n",
        "        pos = nx.spring_layout(G, seed=42)\n",
        "\n",
        "        # ノードのサイズを調整\n",
        "        node_size = [100 + 20 * G.nodes[node]['size'] for node in G.nodes()]\n",
        "\n",
        "        # エッジの太さを調整\n",
        "        edge_width = [0.5 + G[u][v]['weight'] for u, v in G.edges()]\n",
        "\n",
        "          # ネットワーク図の描画\n",
        "        nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color='#3498db', alpha=0.7)\n",
        "        nx.draw_networkx_edges(G, pos, width=edge_width, alpha=0.5, edge_color='#7f8c8d')\n",
        "\n",
        "        # 日本語フォントプロパティの設定\n",
        "        font_prop = FontProperties(fname=font_path)\n",
        "\n",
        "        # ラベルを手動で描画\n",
        "        for node, (x, y) in pos.items():\n",
        "            plt.text(x, y, node, fontproperties=font_prop,\n",
        "                   size=12, ha='center', va='center',\n",
        "                   bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', boxstyle='round,pad=0.3'))\n",
        "\n",
        "\n",
        "        plt.title(\"カテゴリの共起ネットワーク\", fontsize=16)\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"カテゴリネットワーク図を保存しました: {output_file}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"カテゴリネットワーク図の作成中にエラーが発生しました: {e}\")\n",
        "        return False\n",
        "\n",
        "def is_english_tech_term(word):\n",
        "    \"\"\"英語の技術用語かどうかを判定\"\"\"\n",
        "    # 英数字のみで構成されている\n",
        "    if word.isascii() and not word.isdigit():\n",
        "        # 一般的な英語のストップワードリスト\n",
        "        eng_stopwords = {\n",
        "            'the', 'and', 'to', 'of', 'a', 'in', 'for', 'is', 'on', 'that', 'by',\n",
        "            'this', 'with', 'it', 'as', 'are', 'was', 'be', 'at', 'from', 'has',\n",
        "            'have', 'had', 'an', 'but', 'or', 'if', 'they', 'their', 'what', 'which',\n",
        "            'when', 'who', 'how', 'where', 'why', 'not', 'all', 'any', 'can', 'will'\n",
        "        }\n",
        "\n",
        "        additional_stopwords = {\n",
        "            'is', 'do', 'get', 'got', 'vs', 'mini', 'deep', 'lab', 'see',\n",
        "            'go', 'next', 'line', 'mac', 'fp', 'ceo', 'dx',\n",
        "            'pro', 'ui', 'pc', 'de', 'ai', 'openai', 'google', 'apple', 'amazon','microsoft'\n",
        "        }\n",
        "\n",
        "        # 両方のストップワードセットを結合\n",
        "        all_stopwords = eng_stopwords.union(additional_stopwords)\n",
        "\n",
        "        # 小文字に変換して判定\n",
        "        lower_word = word.lower()\n",
        "\n",
        "        # 長さが2以上で、ストップワードではない場合\n",
        "        return len(word) >= 2 and lower_word not in all_stopwords\n",
        "    return False\n",
        "\n",
        "def generate_weekly_report(start_date=None, end_date=None, output_dir=\"reports\",english_only=False):\n",
        "    \"\"\"週間トレンドレポートを生成する\"\"\"\n",
        "    # 日付が指定されていない場合は先週を対象にする\n",
        "    if not start_date or not end_date:\n",
        "        today = datetime.now()\n",
        "        end_date = today.strftime(\"%Y-%m-%d\")\n",
        "        start_date = (today - timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # 日付をYYYYMMDD_YYYYMMDD形式に変換してファイル名用の文字列を生成\n",
        "    start_date_obj = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "    end_date_obj = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "    date_range_str = f\"{start_date_obj.strftime('%Y%m%d')}_{end_date_obj.strftime('%Y%m%d')}\"\n",
        "\n",
        "    print(f\"期間: {start_date} から {end_date} のレポートを生成します\")\n",
        "\n",
        "    # 出力ディレクトリに日付範囲を含める\n",
        "    output_dir = f\"{output_dir}/{date_range_str}\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # データを取得\n",
        "    results = query_database(start_date, end_date)\n",
        "    if not results:\n",
        "        print(\"指定された期間のデータが見つかりませんでした\")\n",
        "        return\n",
        "\n",
        "    # データをDataFrameに変換\n",
        "    df = extract_properties(results)\n",
        "    print(f\"{len(df)}件の記事が見つかりました\")\n",
        "\n",
        "    # 基本的な統計情報を表示\n",
        "    if 'category' in df.columns and not df['category'].isna().all():\n",
        "        print(\"タグ別の記事数:\")\n",
        "        print(df['category'].value_counts())\n",
        "\n",
        "    if 'pickup_type' in df.columns and not df['pickup_type'].isna().all():\n",
        "        print(\"\\nピックアップタイプ別の記事数:\")\n",
        "        print(df['pickup_type'].value_counts())\n",
        "\n",
        "    # ワードクラウドの生成\n",
        "    word_freq = None\n",
        "    if 'content' in df.columns and not df['content'].isna().all():\n",
        "        word_freq = create_word_frequency(df, english_only=english_only)\n",
        "        if word_freq:\n",
        "            generate_wordcloud(\n",
        "                word_freq,\n",
        "                f\"{'英語' if english_only else ''}キーワード頻度 ({start_date} ~ {end_date})\",\n",
        "                f\"{output_dir}/wordcloud.png\",\n",
        "                max_words=100\n",
        "            )\n",
        "            print(\"ワードクラウドを生成しました\")\n",
        "        else:\n",
        "            print(\"警告: 単語頻度データが空のため、ワードクラウドを生成できませんでした\")\n",
        "\n",
        "    # トピックモデリング\n",
        "    topics = {}\n",
        "    if 'content' in df.columns and not df['content'].isna().all() and len(df) >= 5:\n",
        "        topics, doc_topic_dist, vectorizer, lda = create_topic_model_sklearn(df, english_only=english_only)\n",
        "        try:\n",
        "          if topics and doc_topic_dist is not None and lda is not None:\n",
        "              print(\"\\nトピックモデル:\")\n",
        "              for topic_name, words in topics.items():\n",
        "                  print(f\"{topic_name}: {', '.join(words[:7])}\")\n",
        "\n",
        "              plot_topic_distribution_sklearn(doc_topic_dist, topics, lda.n_components, f\"{output_dir}/topic_distribution.png\")\n",
        "        except Exception as e:\n",
        "            print(f\"トピックモデル作成中にエラーが発生しました: {e}\")\n",
        "            topics = {}\n",
        "    else:\n",
        "        topics = {}\n",
        "        if 'content' not in df.columns or df['content'].isna().all():\n",
        "            print(\"コンテンツデータがないため、トピックモデリングをスキップします\")\n",
        "\n",
        "    # カテゴリ（タグ）分布の可視化\n",
        "    plot_category_distribution(df, f\"{output_dir}/category_distribution.png\")\n",
        "\n",
        "\n",
        "    # 時系列分析\n",
        "    if 'date' in df.columns and not df['date'].isna().all():\n",
        "        try:\n",
        "            plot_time_series(df, f\"{output_dir}/time_series.png\")\n",
        "            print(\"時系列グラフを生成しました\")\n",
        "        except Exception as e:\n",
        "            print(f\"時系列グラフ作成中にエラーが発生しました: {e}\")\n",
        "\n",
        "\n",
        "    # カテゴリネットワーク図の生成\n",
        "    category_network_generated = analyze_category_co_occurrence(df, f\"{output_dir}/category_network.png\")\n",
        "    if category_network_generated:\n",
        "        print(\"カテゴリネットワーク図を生成しました\")\n",
        "    else:\n",
        "        print(\"カテゴリネットワーク図の生成をスキップしました\")\n",
        "\n",
        "    # レポート結果をJSONとして保存\n",
        "    category_data = {}\n",
        "    if 'category' in df.columns and not df['category'].isna().all():\n",
        "        category_data = df['category'].value_counts().to_dict()\n",
        "\n",
        "    pickup_data = {}\n",
        "    if 'pickup_type' in df.columns and not df['pickup_type'].isna().all():\n",
        "        pickup_data = df['pickup_type'].value_counts().to_dict()\n",
        "\n",
        "    flag_data = {}\n",
        "    if 'flag' in df.columns and not df['flag'].isna().all():\n",
        "        flag_data = df['flag'].value_counts().to_dict()\n",
        "\n",
        "    keyword_data = {}\n",
        "    if word_freq:\n",
        "        keyword_data = dict(word_freq.most_common(50))\n",
        "\n",
        "    topic_data = {}\n",
        "    if topics:\n",
        "        for topic_name, words in topics.items():\n",
        "            topic_data[topic_name] = words[:10]\n",
        "\n",
        "    report_data = {\n",
        "        \"period\": {\n",
        "            \"start_date\": start_date,\n",
        "            \"end_date\": end_date\n",
        "        },\n",
        "        \"article_count\": len(df),\n",
        "        \"categories\": category_data,\n",
        "        \"pickup_types\": pickup_data,\n",
        "        \"flags\": flag_data,\n",
        "        \"top_keywords\": keyword_data,\n",
        "        \"topics\": topic_data\n",
        "    }\n",
        "\n",
        "    with open(f\"{output_dir}/report_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(report_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"レポートデータをJSONとして保存しました: {output_dir}/report_data.json\")\n",
        "\n",
        "    return {\n",
        "        \"df\": df,\n",
        "        \"report_data\": report_data,\n",
        "        \"output_dir\": output_dir\n",
        "    }\n"
      ],
      "metadata": {
        "id": "JhaKhYFPInP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = \"2025-03-21\"\n",
        "end_date = \"2025-04-17\"\n",
        "\n",
        "output_dir = f\"/content/drive/MyDrive/weekly_report\"\n",
        "report = generate_weekly_report(start_date, end_date, output_dir, english_only=True)"
      ],
      "metadata": {
        "id": "MhyBGeFAItM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VBlM7XiqSRh6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}